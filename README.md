<div align="center">
    <h1>Finger Movements Classification Using VQShape Architecture</h1>
    <h3>Universidad Aut√≥noma de Occidente 2025</h3>
    <p><strong>Lopez Juan Manuel, Botero William, Salamanca Danna</strong></p>
</div>

---

## Nombre de Articulo Base + Repositorio en Github Original

El siguiente trabajo est√° basado principalmente en el uso de la Arquitectura VQShape la cual es la responsable de realizar todo el proceso de reconstrucci√≥n de Series Temporales, entrenada en distintos datasets usando una generalizaci√≥n de formas de las subsecuencias, para as√≠ fomentar el uso de la arquitectura en cualquier √°mbito que se desee trabajar, Desde Electrocardiogramas, hasta emociones del ser humano.

### Enlace al Articulo Original de la Arquitectura VQShape
#### [Abstracted Shapes as Tokens - A Generalizable and Interpretable Model for Time-series Classification](https://arxiv.org/pdf/2411.01006)

### Enlace al Repositorio de Github de VQShape
#### [Repositorio Arquitectura VQShape](https://github.com/YunshiWen/VQShape)

## üß† Descripci√≥n del Modelo
El Modelo VQShape es un modelo de Deep Learning para clasificaci√≥n de Series Temporales, que combina autoencoders convencionales (*Encoder - Decoder*) con una capa de Cuantificaci√≥n Vectorial (*Vector Quantization*) para lograr aprender representaciones dicretas y compactas del tiempo. Su arquitectura transforma la se√±al original en "c√≥digos" discretos (*Embeddings Cuantizados*), lo que permite que esta capture patrones locales y globales, reduciendo el ruido y asi poder mejorar la robustez del clasificador final.

## üöÄ Principales Innovaciones
1. Muchos modelos convencionales, como las CNN (Convolutional Neural Networks) o las MLP (Multi-Layer Perceptron), son considerados *black-box models* porque, aunque pueden ofrecer excelentes resultados, las representaciones internas que producen no son interpretables para los seres humanos. La arquitectura VQShape busca solucionar precisamente este problema: permitir que los humanos comprendan mejor c√≥mo se representan las formas de las subsecuencias en series temporales. Para lograrlo, VQShape utiliza Cuantificaci√≥n Vectorial y un diccionario universal de subsecuencias (*Universal Codebook*), que act√∫a como un conjunto de ‚Äúformas b√°sicas‚Äù aprendidas previamente y que sirven para representar subsecuencias de manera m√°s sencilla, consistente e interpretable.

2. Otro aspecto importante que aborda VQShape es la alta sensibilidad que tienen los modelos tradicionales frente al ruido o a peque√±as variaciones en las se√±ales. En modelos como las CNN o las MLP, cualquier cambio m√≠nimo en la serie puede alterar las representaciones internas y afectar la predicci√≥n. VQShape resuelve esto convirtiendo las subsecuencias en c√≥digos discretos que provienen del diccionario universal. De esta forma, peque√±as fluctuaciones no cambian el c√≥digo asignado, lo que hace que las representaciones sean m√°s estables, m√°s robustas y mucho m√°s consistentes entre diferentes muestras.

3. Adem√°s, VQShape evita uno de los problemas m√°s frecuentes en los modelos basados en autoencoders, como lo es el colapso del espacio latente. En modelos continuos, el encoder suele proyectar muchas subsecuencias a zonas muy cercanas, perdiendo diversidad. Con la Cuantificaci√≥n Vectorial, VQShape obliga al modelo a utilizar distintos c√≥digos del diccionario, manteniendo una representaci√≥n m√°s equilibrada y evitando que todos los patrones se agrupen en un solo lugar. Esto permite que las subsecuencias se distribuyan mejor y capturen diferentes formas presentes en la serie temporal.

4. Otro punto clave es que VQShape utiliza un diccionario compartido entre todas las muestras del dataset, lo que garantiza que subsecuencias parecidas siempre sean representadas de forma similar. En modelos tradicionales, dos se√±ales casi iguales pueden terminar con representaciones muy diferentes debido a las caracter√≠sticas continuas aprendidas. Con el Universal Codebook, todas las series temporales usan las mismas ‚Äúformas base‚Äù, lo que mejora la consistencia entre las representaciones y facilita comparar se√±ales o entender patrones globales del dataset.

5. Finalmente, VQShape mejora la generalizaci√≥n del modelo al obligar a representar las subsecuencias de forma m√°s abstracta y menos dependiente de detalles irrelevantes. Al usar c√≥digos discretos, el modelo no se enfoca en peque√±as diferencias entre muestras, sino que captura patrones m√°s generales y recurrentes. Esto reduce el riesgo de sobreajuste y hace que el modelo funcione mejor en se√±ales nuevas, incluso si presentan cambios ligeros respecto a las muestras de entrenamiento.

## Resumen Te√≥rico de la Arquitectura

<p align="center">
  <img src="Arquitectura.jpg" alt="Arquitectura" width="70%">
</p>


La arquitectura VQShape est√° dise√±ada para representar series temporales mediante formas discretas que se aprenden directamente del espacio latente. El proceso comienza con una serie temporal univariada, a la cual primero se le aplica Instance Normalization y luego se divide en peque√±os segmentos llamados patches. Cada patch es transformado a un espacio de mayor dimensi√≥n usando una capa lineal con positional embedding, de manera que el modelo pueda capturar la informaci√≥n temporal y la posici√≥n relativa dentro de la serie. Estos embeddings se introducen en un Time-series Encoder basado en Transformer, que se encarga de extraer las caracter√≠sticas globales de la se√±al.

A partir de estas caracter√≠sticas, el Attribute Decoder predice un conjunto de atributos (*Representaci√≥n Abstracta de la Forma, Desplazamiento, Escala, Posici√≥n Inicial Relativa y Longitud Relativa*) que describen la forma de cada subsecuencia. En paralelo, el modelo genera una representaci√≥n latente continua que ser√° cuantizada mediante un codebook discreto. Esta cuantizaci√≥n es lo que permite que cada subsecuencia sea representada por uno de los c√≥digos del diccionario, haciendo que la representaci√≥n final sea m√°s estable, interpretable y robusta. El Desplazamiento y la Escala junto con el c√≥digo seleccionado son enviados a un Shape Decoder basado en CNN, que reconstruye la forma estimada de la subsecuencia y permite comparar esta forma con una forma objetivo mediante la p√©rdida de forma (L_s).

Adem√°s, el modelo incluye una operaci√≥n de interpolaci√≥n que ajusta la forma generada dependiendo de los atributos predichos, especialmente la longitud, asegurando que las subsecuencias reconstruidas tengan proporciones coherentes con el segmento original de la serie temporal. En la parte final, el conjunto de c√≥digos y atributos se vuelve a introducir en un Attribute Encoder y despu√©s en un Time-series Decoder (tambi√©n basado en Transformer) para reconstruir la serie temporal completa. De esta manera, VQShape no solo aprende representaciones discretas e interpretables, sino que tambi√©n es capaz de reconstruir la se√±al original, lo cual garantiza que el espacio latente conserve la informaci√≥n esencial de la serie.

Por ultimo, una vez se tiene la serie reconstruida, y se compara con la original para ver que tan bien se reconstrusye, esta serie para por una capa MLP para el proceso de Clasificaci√≥n el cual nos permite ver que tan bien est√° reconstruyendo el modelo, respecto a la reconstrucci√≥n de la serie que hace en todo el proceso de la arquitectura.

##### A continuaci√≥n, se va a explicar como hacer el proceso de instalaci√≥n de este Repositorio, para poder trabajar un modelo de clasificaci√≥n, usando la arquitectura vista de VQShape.

## üìã Gu√≠a de Instalaci√≥n y Ejecuci√≥n

### 1. Clonar el repositorio
```bash
git clone https://github.com/willyoung21/VQShape_finger_classification.git
cd VQShape_finger_classification
```

### 2. Preparar los checkpoints de VQShape (‚ö†Ô∏è IMPORTANTE)

El repositorio **no contiene directamente** la carpeta `/checkpoints`, pero s√≠ incluye un archivo comprimido:
 
**`uea_dim256_codebook512.zip`**

**Debes:**
- Descomprimirlo dentro de la carpeta `VQShape`
- Esto generar√° autom√°ticamente la ruta necesaria: `VQShape/checkpoints/uea_dim256_codebook512/VQShape.ckpt`

**La estructura final debe quedar as√≠:**
```
VQShape/
 ‚îî‚îÄ‚îÄ checkpoints/
      ‚îî‚îÄ‚îÄ uea_dim256_codebook512/
           ‚îî‚îÄ‚îÄ VQShape.ckpt
```

> **Nota:** El modelo de clasificaci√≥n EEG (`best_eeg_classifier.pt`) **s√≠ est√° incluido** en el repositorio, dentro de:
> ```
> Modelo/best_eeg_classifier.pt
> ```
> Por lo tanto, no requiere instalaci√≥n adicional.

---

### 3. Ejecutar la aplicaci√≥n con Docker üê≥

El proyecto incluye un `Dockerfile` y un `docker-compose.yml`, por lo que solo necesitas ejecutar:
```bash
docker compose up --build
```

Esto construir√° la imagen autom√°ticamente:

‚úÖ Instala Python 3.11  
‚úÖ Instala dependencias desde `app/requirements.txt`  
‚úÖ Copia el c√≥digo dentro del contenedor  
‚úÖ Configura el `PYTHONPATH` para VQShape  

Cuando termine el build, la app de **Streamlit** quedar√° disponible en:

üåê **[http://localhost:8501](http://localhost:8501)**

---

### 4. ¬øNecesito instalar `requirements.txt` localmente?

**‚ùå No.**

Debido a Docker:

- Todas las dependencias se instalan **dentro del contenedor**
- Tu m√°quina local **no necesita instalar nada** (ni Python ni pip)

**Solo si quisieras ejecutar la app sin Docker**, entonces s√≠ tendr√≠as que instalar:
```bash
pip install -r app/requirements.txt
```

Pero **no es necesario** para el flujo principal, ya que Docker gestiona todo.

---

## üîß ¬øC√≥mo se cargan los pesos?

El archivo `app.py` carga dos modelos:

### 1Ô∏è‚É£ VQShape (tokenizador pretrained)

**Ruta generada despu√©s de descomprimir el ZIP:**
```
VQShape/checkpoints/uea_dim256_codebook512/VQShape.ckpt
```

**El modelo se carga as√≠:**
```python
lit = LitVQShape.load_from_checkpoint(CHECKPOINT, map_location="cpu")
base_model = lit.model
```

> **Nota:** Luego se congela porque solo se utiliza para **inferencia**, no para entrenamiento.

---

### 2Ô∏è‚É£ Clasificador EEG (linear head)

**Este s√≠ viene dentro del repo:**
```
Modelo/best_eeg_classifier.pt
```

**Y se carga con:**
```python
clf.load_state_dict(torch.load(CLASSIFIER_WEIGHTS, map_location="cpu"))
```

---

## üß† ¬øC√≥mo funciona la inferencia?

### üìÇ Datos de ejemplo

En el repositorio, dentro de la carpeta `VQShape/datos/FingerMovements`, est√°n dos archivos `.ts`:

- `FingerMovements_TEST.ts` ‚úÖ **(√∫salo para pruebas)**
- `FingerMovements_TRAIN.ts` ‚ö†Ô∏è **(no usar para inferencia)**

---

### üîÑ Flujo de procesamiento

1. **Subes un archivo `.ts`** con se√±ales EEG (por ejemplo, `FingerMovements_TEST.ts`)

2. **Se toma una muestra (trial)** del archivo

3. **Interpolaci√≥n:** Se ajusta de **50 ‚Üí 512 timesteps** para mantener compatibilidad con VQShape

4. **Tokenizaci√≥n:** VQShape convierte cada canal EEG en un **histograma de c√≥digos** (512 dimensiones)

5. **Promediado:** Se promedian los histogramas de todos los canales

6. **Clasificaci√≥n:** Ese vector de 512 valores entra al clasificador lineal

7. **Predicci√≥n final:** El modelo predice:
   - üëà **LEFT** 
   - üëâ **RIGHT**

---

<div align="center">
    <p>Made with ‚ù§Ô∏è by the UAO Team</p>
</div>