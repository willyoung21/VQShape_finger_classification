<div align="center">
    <h1>Finger Movements Classification Using VQShape Architecture</h1>
    <h3>Universidad AutÃ³noma de Occidente 2025</h3>
    <p><strong>Lopez Juan Manuel, Botero William, Salamanca Danna</strong></p>
</div>

---

## ğŸ“‹ GuÃ­a de InstalaciÃ³n y EjecuciÃ³n

### 1. Clonar el repositorio
```bash
git clone https://github.com/willyoung21/VQShape_finger_classification.git
cd VQShape_finger_classification
```

### 2. Preparar los checkpoints de VQShape (âš ï¸ IMPORTANTE)

El repositorio **no contiene directamente** la carpeta `/checkpoints`, pero sÃ­ incluye un archivo comprimido:
 
**`uea_dim256_codebook512.zip`**

**Debes:**
- Descomprimirlo dentro de la carpeta `VQShape`
- Esto generarÃ¡ automÃ¡ticamente la ruta necesaria: `VQShape/checkpoints/uea_dim256_codebook512/VQShape.ckpt`

**La estructura final debe quedar asÃ­:**
```
VQShape/
 â””â”€â”€ checkpoints/
      â””â”€â”€ uea_dim256_codebook512/
           â””â”€â”€ VQShape.ckpt
```

> **Nota:** El modelo de clasificaciÃ³n EEG (`best_eeg_classifier.pt`) **sÃ­ estÃ¡ incluido** en el repositorio, dentro de:
> ```
> Modelo/best_eeg_classifier.pt
> ```
> Por lo tanto, no requiere instalaciÃ³n adicional.

---

### 3. Ejecutar la aplicaciÃ³n con Docker ğŸ³

El proyecto incluye un `Dockerfile` y un `docker-compose.yml`, por lo que solo necesitas ejecutar:
```bash
docker compose up --build
```

Esto construirÃ¡ la imagen automÃ¡ticamente:

âœ… Instala Python 3.11  
âœ… Instala dependencias desde `app/requirements.txt`  
âœ… Copia el cÃ³digo dentro del contenedor  
âœ… Configura el `PYTHONPATH` para VQShape  

Cuando termine el build, la app de **Streamlit** quedarÃ¡ disponible en:

ğŸŒ **[http://localhost:8501](http://localhost:8501)**

---

### 4. Â¿Necesito instalar `requirements.txt` localmente?

**âŒ No.**

Debido a Docker:

- Todas las dependencias se instalan **dentro del contenedor**
- Tu mÃ¡quina local **no necesita instalar nada** (ni Python ni pip)

**Solo si quisieras ejecutar la app sin Docker**, entonces sÃ­ tendrÃ­as que instalar:
```bash
pip install -r app/requirements.txt
```

Pero **no es necesario** para el flujo principal, ya que Docker gestiona todo.

---

## ğŸ”§ Â¿CÃ³mo se cargan los pesos?

El archivo `app.py` carga dos modelos:

### 1ï¸âƒ£ VQShape (tokenizador pretrained)

**Ruta generada despuÃ©s de descomprimir el ZIP:**
```
VQShape/checkpoints/uea_dim256_codebook512/VQShape.ckpt
```

**El modelo se carga asÃ­:**
```python
lit = LitVQShape.load_from_checkpoint(CHECKPOINT, map_location="cpu")
base_model = lit.model
```

> **Nota:** Luego se congela porque solo se utiliza para **inferencia**, no para entrenamiento.

---

### 2ï¸âƒ£ Clasificador EEG (linear head)

**Este sÃ­ viene dentro del repo:**
```
Modelo/best_eeg_classifier.pt
```

**Y se carga con:**
```python
clf.load_state_dict(torch.load(CLASSIFIER_WEIGHTS, map_location="cpu"))
```

---

## ğŸ§  Â¿CÃ³mo funciona la inferencia?

### ğŸ“‚ Datos de ejemplo

En el repositorio, dentro de la carpeta `VQShape/datos/FingerMovements`, estÃ¡n dos archivos `.ts`:

- `FingerMovements_TEST.ts` âœ… **(Ãºsalo para pruebas)**
- `FingerMovements_TRAIN.ts` âš ï¸ **(no usar para inferencia)**

---

### ğŸ”„ Flujo de procesamiento

1. **Subes un archivo `.ts`** con seÃ±ales EEG (por ejemplo, `FingerMovements_TEST.ts`)

2. **Se toma una muestra (trial)** del archivo

3. **InterpolaciÃ³n:** Se ajusta de **50 â†’ 512 timesteps** para mantener compatibilidad con VQShape

4. **TokenizaciÃ³n:** VQShape convierte cada canal EEG en un **histograma de cÃ³digos** (512 dimensiones)

5. **Promediado:** Se promedian los histogramas de todos los canales

6. **ClasificaciÃ³n:** Ese vector de 512 valores entra al clasificador lineal

7. **PredicciÃ³n final:** El modelo predice:
   - ğŸ‘ˆ **LEFT** 
   - ğŸ‘‰ **RIGHT**

---

<div align="center">
    <p>Made with â¤ï¸ by the UAO Team</p>
</div>