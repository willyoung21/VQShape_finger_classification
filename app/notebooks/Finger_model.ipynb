{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Capa adicional de clasificacion de finger movements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Realizamos una capa adicional de clasificación sobre las formas tokenizadas generadas por VQShape. Aunque VQShape se entrena principalmente como un extractor de características — proporcionando una representación en forma de tokens y un histograma de códigos para series de tiempo — no incluye de por sí una cabeza de clasificación supervisada.\n",
        "\n",
        "Aprovechamos ese modelo pre-entrenado como feature extractor, y sobre su salida aplicamos un clasificador lineal que permite distinguir los movimientos de dedos. El entrenamiento fue realizado en un entorno de Google Colab, aprovechando una GPU para acelerar el proceso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como estamos en un entorno colab clonamos el repositorio original y nos vamos a el directorio VQShape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mneZ1RzbrDvd",
        "outputId": "1e67d131-2294-4232-d4e0-d17056de5df0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'VQShape' already exists and is not an empty directory.\n",
            "/content/VQShape\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/YunshiWen/VQShape.git\n",
        "%cd VQShape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instalamos algunas librerias necesarias para la cabeza de clasificacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBcj2imqsk8e"
      },
      "outputs": [],
      "source": [
        "!pip install lightning\n",
        "!pip install sktime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cargamos el modelo VQShape preentrenado\n",
        "\n",
        "Creamos la carpeta \"checkpoints\" y dentro de esa carpeta creamos una sub carpeta llamada \"uea_dim256_codebook512\" para cargar pesos .ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xc1uTMK9rizn"
      },
      "outputs": [],
      "source": [
        "from vqshape.pretrain import LitVQShape\n",
        "import torch\n",
        "\n",
        "checkpoint = \"checkpoints/uea_dim256_codebook512/VQShape.ckpt\" # carga los pesos .ckpt\n",
        "lit = LitVQShape.load_from_checkpoint(checkpoint, map_location=\"cuda\") # Se mueve todo a GPU\n",
        "base_model = lit.model\n",
        "base_model.eval()\n",
        "\n",
        "for p in base_model.parameters(): # Se congela (requires_grad = False) porque no  \n",
        "    p.requires_grad = False      #vamos a entrenar VQShape, solo lo usamos como “feature extractor”\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cargamos dataset Finger Movements (sktime)\n",
        "\n",
        "Utilizamos sktime para cargar los archivos .ts oficiales del dataset Finger Movements.\n",
        "Este dataset contiene señales multivariadas de EEG para dos clases:\n",
        "\n",
        "- left\n",
        "- right\n",
        "\n",
        "Convertimos las etiquetas de texto a enteros (0 y 1) para entrenar el clasificador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GiHJuAwvvgeE"
      },
      "outputs": [],
      "source": [
        "from sktime.datasets import load_from_tsfile\n",
        "import numpy as np\n",
        "\n",
        "X_train, y_train = load_from_tsfile(\"/content/FingerMovements_TRAIN.ts\", return_data_type=\"numpy3D\")\n",
        "X_test, y_test   = load_from_tsfile(\"/content/FingerMovements_TEST.ts\",  return_data_type=\"numpy3D\")\n",
        "\n",
        "label_map = {\"left\": 0, \"right\": 1}\n",
        "y_train = np.array([label_map[y] for y in y_train])\n",
        "y_test  = np.array([label_map[y] for y in y_test])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imprimimos el modelo para tener una guia clara"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4_G7eD10QVG"
      },
      "outputs": [],
      "source": [
        "print(base_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocesamiento de señales y extracción de histogramas con VQShape\n",
        "\n",
        "Antes de alimentar los datos al clasificador, es necesario preprocesar cada señal y obtener su representación discreta con VQShape. Para ello se definen dos funciones:\n",
        "\n",
        "1. preprocess_signal() – Adaptación de la señal al modelo VQShape\n",
        "Esta función:\n",
        "\n",
        "- Convierte la muestra a tensor en GPU.\n",
        "- Interpola cada canal a longitud 512 (requisito de VQShape).\n",
        "- Reorganiza la matriz para tratar cada canal como una serie temporal independiente.\n",
        "\n",
        "2. get_histogram() – Obtención del embedding discreto (histograma del codebook)\n",
        "Aquí se usa el modelo VQShape en modo tokenize para:\n",
        "\n",
        "- Obtener los códigos cuantizados de cada canal.\n",
        "- Extraer el histograma de activaciones del codebook.\n",
        "- Promediar los histogramas de los 28 canales para obtener un vector de 512 dimensiones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "\n",
        "def preprocess_signal(x):\n",
        "    x = torch.tensor(x).float().cuda()       # (28,50)\n",
        "    x = F.interpolate(x.unsqueeze(0), size=512, mode='linear').squeeze(0)  # (28,512)\n",
        "    x = rearrange(x, 'c t -> (c) t')         # (28,512) → 28 univariates\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_histogram(x):\n",
        "    reps, _ = base_model(x, mode=\"tokenize\")\n",
        "    hist = reps[\"histogram\"]   # (28, codebook_size)\n",
        "    hist = hist.float().mean(dim=0)    # → vector (codebook_size,)\n",
        "    return hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cálculo masivo de histogramas para todo el dataset\n",
        "\n",
        "Para acelerar el entrenamiento, se precomputan los histogramas de todas las muestras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Pre-calcula TODOS los histogramas \n",
        "def get_features(X_data):\n",
        "    base_model.eval()\n",
        "    all_hists = []\n",
        "    print(f\"Calculando {len(X_data)} histogramas...\")\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(X_data)):\n",
        "            x = preprocess_signal(X_data[i]) \n",
        "            hist = get_histogram(x)          \n",
        "            all_hists.append(hist)\n",
        "    return torch.stack(all_hists) # Forma: (N_samples, 512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extracción de histogramas + definición del clasificador\n",
        "\n",
        "Para cada serie temporal del dataset aplicamos:\n",
        "- preprocess_signal: normalización y segmentación.\n",
        "- get_histogram: codificación a tokens VQ y construcción de histogramas (512 bins).\n",
        "\n",
        "El resultado (H_train, H_test) es un vector de longitud 512 por muestra:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau # Importamos el scheduler\n",
        "import numpy as np\n",
        "import copy # Para guardar el mejor modelo\n",
        "\n",
        "\n",
        "# Imprimimos los histogramas\n",
        "print(\"--- Paso 1: Calculando Features ---\")\n",
        "H_train = get_features(X_train)\n",
        "H_test  = get_features(X_test)\n",
        "y_train_tensor = torch.tensor(y_train).long().cuda()\n",
        "y_test_tensor  = torch.tensor(y_test).long().cuda()\n",
        "print(\"Features calculados.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo de clasificación\n",
        "\n",
        "Implementamos un clasificador simple y lineal:\n",
        "- Entrada: histograma de 512 dimensiones.\n",
        "- Salida: dos clases (left / right).\n",
        "\n",
        "Este clasificador se entrena desde cero, usando VQShape únicamente como extractor de tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definimos  el Clasificador\n",
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self, hist_dim=512):\n",
        "        super().__init__()\n",
        "        self.net = nn.Linear(hist_dim, 2)\n",
        "\n",
        "    def forward(self, h):\n",
        "        return self.net(h)\n",
        "\n",
        "clf = SimpleClassifier(hist_dim=512).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizador, scheduler y dataloaders\n",
        "\n",
        "Se utiliza:\n",
        "\n",
        "- Adam con lr=1e-3 y weight_decay=1e-5 (igual que el repo original).\n",
        "- CrossEntropyLoss para clasificación binaria en logits.\n",
        "- Scheduler ReduceLROnPlateau: reduce el LR si no mejora el accuracy en test, ayudando a estabilizar el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definimos el Optimizador y Scheduler\n",
        "# El repo usa weight_decay=1e-5 y lr=1e-3\n",
        "optimizer = torch.optim.Adam(clf.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Usamos un scheduler para reducir el LR si el test_acc deja de mejorar\n",
        "# Esto ayuda a \"afinar\" el modelo y evitar sobreajuste\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entrenamiento con early stopping y LR scheduler\n",
        "\n",
        "Durante cada época se calculan:\n",
        "\n",
        "- accuracy en entrenamiento\n",
        "- accuracy en test\n",
        "\n",
        "Mecanismos implementados:\n",
        "\n",
        "- Scheduler: Reduce la tasa de aprendizaje al no mejorar el accuracy.\n",
        "- Early Stopping: Si el modelo no mejora durante 30 épocas, se detiene el entrenamiento automáticamente.\n",
        "- Guardado del mejor modelo: Se almacena en memoria (best_model_state) para cargarlo al final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQNNvA5yGbD3",
        "outputId": "edca26bc-3e0f-496c-bffb-75ee47369bc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Paso 1: Calculando Features ---\n",
            "Calculando 316 histogramas...\n",
            "Calculando 100 histogramas...\n",
            "Features calculados.\n",
            "\n",
            "--- Paso 2: Iniciando entrenamiento optimizado ---\n",
            "Epoch 001 | Train Acc: 0.506 | Test Acc: 0.510 (¡Nuevo Mejor!)\n",
            "Epoch 010 | Train Acc: 0.630 | Test Acc: 0.510\n",
            "Epoch 020 | Train Acc: 0.684 | Test Acc: 0.540\n",
            "Epoch 030 | Train Acc: 0.680 | Test Acc: 0.520\n",
            "Epoch 040 | Train Acc: 0.703 | Test Acc: 0.490\n",
            "Parando temprano por falta de mejora.\n",
            "\n",
            "Entrenamiento finalizado.\n",
            "Mejor Test Accuracy: 0.590 (en época 9)\n"
          ]
        }
      ],
      "source": [
        "# DataLoaders\n",
        "BATCH_SIZE = 32\n",
        "train_dataset = TensorDataset(H_train, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataset = TensorDataset(H_test, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Bucle de Entrenamiento con Early Stopping y Scheduler \n",
        "print(\"\\n--- Paso 2: Iniciando entrenamiento optimizado ---\")\n",
        "\n",
        "# Variables para guardar el mejor modelo\n",
        "best_test_acc = 0.0\n",
        "best_epoch = 0\n",
        "best_model_state = None # Guardaremos los \"pesos\" del mejor modelo\n",
        "MAX_EPOCHS = 150\n",
        "\n",
        "for epoch in range(MAX_EPOCHS):\n",
        "    clf.train()\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for h_batch, y_batch in train_loader:\n",
        "        logits = clf(h_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = logits.argmax(1)\n",
        "        train_correct += (preds == y_batch).sum().item()\n",
        "        train_total += y_batch.size(0)\n",
        "\n",
        "    train_acc = train_correct / train_total\n",
        "\n",
        "    # Bucle de Evaluación (Validación) \n",
        "    clf.eval()\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    with torch.no_grad():\n",
        "        for h_batch, y_batch in test_loader:\n",
        "            logits = clf(h_batch)\n",
        "            preds = logits.argmax(1)\n",
        "            test_correct += (preds == y_batch).sum().item()\n",
        "            test_total += y_batch.size(0)\n",
        "\n",
        "    test_acc = test_correct / test_total\n",
        "\n",
        "    # Lógica de Early Stopping y Scheduler \n",
        "\n",
        "    # El scheduler se alimenta con el accuracy de test\n",
        "    scheduler.step(test_acc)\n",
        "\n",
        "    if test_acc > best_test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "        best_model_state = copy.deepcopy(clf.state_dict()) # Guarda una copia de los pesos\n",
        "\n",
        "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "             print(f\"Epoch {epoch+1:03d} | Train Acc: {train_acc:.3f} | Test Acc: {test_acc:.3f} (¡Nuevo Mejor!)\")\n",
        "    else:\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "             print(f\"Epoch {epoch+1:03d} | Train Acc: {train_acc:.3f} | Test Acc: {test_acc:.3f}\")\n",
        "\n",
        "    # Parada temprana si no mejora en 30 épocas\n",
        "    if (epoch - best_epoch) > 30:\n",
        "        print(\"Parando temprano por falta de mejora.\")\n",
        "        break\n",
        "\n",
        "print(f\"\\nEntrenamiento finalizado.\")\n",
        "print(f\"Mejor Test Accuracy: {best_test_acc:.3f} (en época {best_epoch+1})\")\n",
        "\n",
        "# Carga los mejores pesos en el modelo\n",
        "if best_model_state:\n",
        "    clf.load_state_dict(best_model_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Guardamos el mejor modelo\n",
        "\n",
        "Al finalizar el entrenamiento guardamos los pesos del mejor modelo utilizando\n",
        "PyTorch (.pt) para poder reutilizarlo en inferencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqcp3wR3sV9v",
        "outputId": "ea1a1fdf-d989-4700-c507-9ad1a5bd0431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo guardado como best_eeg_classifier.pt\n"
          ]
        }
      ],
      "source": [
        "# Guardar el mejor modelo \n",
        "torch.save(best_model_state, \"best_eeg_classifier.pt\")\n",
        "print(\"Modelo guardado como best_eeg_classifier.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cargamos el modelo entrenado para inferencia\n",
        "\n",
        "Se reconstruye el clasificador y se cargan los pesos entrenados.\n",
        "El modelo se coloca en modo evaluación (eval())."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_atjnpJNPKu",
        "outputId": "3f981384-c069-4013-cae2-46877b0415e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SimpleClassifier(\n",
              "  (net): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf = SimpleClassifier(hist_dim=512).cuda()\n",
        "clf.load_state_dict(torch.load(\"best_eeg_classifier.pt\"))\n",
        "clf.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejemplo de predicción en un índice del test\n",
        "\n",
        "Tomamos un ejemplo del conjunto de test, lo procesamos igual que en entrenamiento y obtenemos la predicción del clasificador.\n",
        "\n",
        "Se compara la clase predicha vs. la etiqueta real para validar el correcto funcionamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87_a4qtENRLY",
        "outputId": "3e88b132-4c6a-4c83-d157-aad8736a0fa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicción: right\n",
            "Etiqueta real: right\n"
          ]
        }
      ],
      "source": [
        "i = 0   # índice\n",
        "\n",
        "with torch.no_grad():\n",
        "    x = preprocess_signal(X_test[i])\n",
        "    hist = get_histogram(x)\n",
        "    pred = clf(hist.unsqueeze(0)).argmax(1).item()\n",
        "\n",
        "print(\"Predicción:\", \"right\" if pred == 1 else \"left\")\n",
        "print(\"Etiqueta real:\", \"right\" if y_test[i] == 1 else \"left\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
